{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file D:\\Python3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys)  #设置系统的编码为utf8，便于输入中文,>3.3版本的写法\n",
    "\n",
    "def getlocation(name):#调用百度API查询位置\n",
    "    bdurl='http://api.map.baidu.com/geocoder/v2/?address='\n",
    "    output='json'\n",
    "    ak='bUXeEu2f0Yf3q3DTGPTIYqW6XOp6z78U'  #输入申请的密匙\n",
    "    callback='showLocation'\n",
    "    uri=bdurl+name+'&output=t'+output+'&ak='+ak+'&callback='+callback\n",
    "    res=requests.get(uri)\n",
    "    s=BeautifulSoup(res.text)\n",
    "    lng=s.find('lng')\n",
    "    lat=s.find('lat')\n",
    "    if lng:\n",
    "        return lng.get_text()+','+lat.get_text()\n",
    "\n",
    "#页面访问头\n",
    "header = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "'Accept-Encoding':'gzip, deflate',\n",
    "'Accept-Language':'zh-CN,zh;q=0.9',\n",
    "'Cache-Control':'max-age=0',\n",
    "'Connection':'keep-alive',\n",
    "'Cookie':'navCtgScroll=0; _lxsdk_cuid=1639f0b1318c8-0041da13dff894-d35346d-1fa400-1639f0b1319c8; _lxsdk=1639f0b1318c8-0041da13dff894-d35346d-1fa400-1639f0b1319c8; _hc.v=58389d39-ff71-2061-f8a5-9fdfcd24bc60.1527381693; s_ViewType=10; ua=dpuser_9757271275; ctu=996e5600e78c8cf16de1bb5d20c3a83d08d75173635324bdce6d80f033fd09cb; cy=5; cye=nanjing; _lx_utm=utm_source%3DBaidu%26utm_medium%3Dorganic; _lxsdk_s=163a42e1606-845-ab6-de1%7C%7C53',\n",
    "'Host':'www.dianping.com',\n",
    "'Upgrade-Insecure-Requests':'1',\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36'\n",
    "}\n",
    "\n",
    "shop_addr=[]  #存放商家地址\n",
    "shop_n=[]    #存放商家名字\n",
    "loc_n=[]     #存放商家地址\n",
    "\n",
    "#访问的页面数范围\n",
    "page=list(range(1,51,1))\n",
    "\n",
    "#访问的页面网址\n",
    "url='http://www.dianping.com/dalishi/ch10/r12465p'#古城\n",
    "#循环遍历所有页面\n",
    "for i in page:\n",
    "    html1 = requests.get(url+str(i), headers=header).content#m6是可外卖\n",
    "    soup1 = BeautifulSoup(html1, \"html.parser\")\n",
    "\n",
    "    # 提取商家名字\n",
    "    shop_names = soup1.find_all('div', class_='tit')\n",
    "    for s in shop_names:\n",
    "        shop_n.append(s.h4.string)\n",
    "\n",
    "    #提取商家评分\n",
    "    ''' comment_scores=soup1.find_all('span', class_='comment-list')\n",
    "    for spantag in comment_scores:\n",
    "        blist = spantag.find_all('b')  # 在每个span标签下,查找所有的b标签\n",
    "        if len(blist)==0:\n",
    "            comment_s.append(1)\n",
    "            continue\n",
    "        print(blist[0].string+' '+blist[1].string+' '+blist[2].string)\n",
    "        score=(float(blist[0].string)+float(blist[1].string)+float(blist[2].string))/3\n",
    "        comment_s.append(score)'''\n",
    "\n",
    "    # 提取商家地址\n",
    "    shop_address = soup1.find_all('span', class_='addr')\n",
    "    for a in shop_address:\n",
    "        shop_addr.append(a.string)\n",
    "        loc = getlocation(a.string)\n",
    "        loc_n.append(loc)\n",
    "\n",
    "    #随机睡眠一段时间，再去爬取数据，模拟人为浏览，防止ip被封\n",
    "    time.sleep(3 + random.uniform(1, 3))\n",
    "\n",
    "#将信息写入csv文件\n",
    "import pandas as pd\n",
    "\n",
    "#字典中的key值即为csv中列名\n",
    "dataframe = pd.DataFrame({'shop_name':shop_n,'shop_address':shop_addr,'location':loc_n})\n",
    "\n",
    "#将DataFrame存储为csv,index表示是否显示行名，default=True\n",
    "dataframe.to_csv(\"GuCheng.csv\",index=False,sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
